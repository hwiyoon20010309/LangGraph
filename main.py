# -*- coding: utf-8 -*-
"""
EdTech íˆ¬ì íŒŒì´í”„ë¼ì¸ (Flowchart ê¸°ë°˜ / Tavily í†µí•© RAG)
- Flow:
  Start -> WebCrawling -> Filtering -> SelectOne
         -> TechSummary -> MarketEval -> CompAnalysis -> InvestDecision -> (Hold -> SelectOne | Report -> End)

ê¸°ëŠ¥:
- [ìˆ˜ì •] Tavily ê²€ìƒ‰ìœ¼ë¡œ 'í•œêµ­ êµìœ¡ ìŠ¤íƒ€íŠ¸ì—…' ë¦¬ìŠ¤íŠ¸ ë° ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ FAISS RAG ì¸ë±ìŠ¤
- ìŠ¤íƒ€íŠ¸ì—… í›„ë³´ ìë™ ì¶”ì¶œ(LLM) + ê°„ë‹¨ ì„ ì • ì ìˆ˜(í‚¤ì›Œë“œ ê¸°ë°˜)
- 6ê°œ í‰ê°€ì˜ì—­(ê¸°ìˆ ë ¥, í•™ìŠµì„±ê³¼, ì‹œì¥ì„±, ê²½ìŸë ¥, ë¦¬ìŠ¤í¬, ì„±ì¥ê°€ëŠ¥ì„±) LLM ì ìˆ˜í™”
- ìµœì¢… íˆ¬ìíŒë‹¨ + PDF ë³´ê³ ì„œ ìƒì„±

í•„ìš”:
- pip install langgraph langchain langchain-openai langchain-community faiss-cpu tiktoken requests beautifulsoup4 reportlab python-dotenv rich
- .env:
    OPENAI_API_KEY=sk-...
    TAVILY_API_KEY=tvly-...   (í•„ìˆ˜: Tavily API í‚¤ê°€ ì—†ìœ¼ë©´ ì´ ì½”ë“œëŠ” ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)
"""

import os, re, json, requests
import time
import traceback
from typing import TypedDict, Literal, List, Dict, Optional
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from rich import print

# LangGraph / LangChain
from langgraph.graph import StateGraph, END
# [ğŸ’¡ğŸ’¡ğŸ’¡] SqliteSaver import êµ¬ë¬¸ ì‚­ì œë¨
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# PDF
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet


# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# =========================
from dotenv import load_dotenv
import os

# .env íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")
LANGCHAIN_PROJECT = os.getenv("LANGCHAIN_PROJECT")

# ê¸°ë³¸ ì²´í¬
if not OPENAI_API_KEY:
    raise RuntimeError("âŒ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. (.env íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ í™•ì¸)")
else:
    print("âœ… OPENAI_API_KEY ë¡œë“œ ì™„ë£Œ")

# [ìˆ˜ì •] TavilyëŠ” ì´ì œ ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ì…ë‹ˆë‹¤.
if not TAVILY_API_KEY:
    raise RuntimeError("âŒ í™˜ê²½ë³€ìˆ˜ TAVILY_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ íŒŒì´í”„ë¼ì¸ì€ Tavily ê²€ìƒ‰ì— ì˜ì¡´í•©ë‹ˆë‹¤.")
else:
    print("âœ… TAVILY_API_KEY ë¡œë“œ ì™„ë£Œ")

# LangChain Tracing ì„¤ì • í™•ì¸ìš© ì¶œë ¥ (ì„ íƒ)
if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print(f"ğŸ§  LangChain tracing í™œì„±í™”ë¨ â€” í”„ë¡œì íŠ¸: {LANGCHAIN_PROJECT}")


# =========================
# ìƒíƒœ ìŠ¤í‚¤ë§ˆ (ê³µìœ  State)
# =========================
class AgentState(TypedDict, total=False):
    # í›„ë³´/ì„ ì •
    candidates: List[Dict]
    filtered: List[Dict]
    current_candidate: Dict
    current_idx: int
    loop_count: int

    # 6ê°œ ì ìˆ˜ (0~100)
    technology_score: int
    learning_effectiveness_score: int
    market_score: int
    competition_score: int
    risk_score: int
    growth_potential_score: int

    # ë¶„ì„ ê·¼ê±°
    technology_analysis_evidence: str
    learning_effectiveness_analysis_evidence: str
    market_analysis_evidence: str
    competition_analysis_evidence: str
    risk_analysis_evidence: str
    growth_potential_analysis_evidence: str

    # íŒë‹¨/ì‚°ì¶œ
    final_judge: Literal["íˆ¬ì", "ë³´ë¥˜"]
    report: str
    pdf_path: str

    # RAG ì¸ë±ìŠ¤ ê²½ë¡œ
    vectorstore_path: str


# =========================
# í‰ê°€ ê¸°ì¤€(ì°¸ì¡°ìš©) / ì„ê³„ì¹˜
# =========================
EVALUATION_CRITERIA = {
    "technology": {
        "ì œí’ˆì´ êµìœ¡ ë¬¸ì œë¥¼ ëª…í™•í•˜ê²Œ í•´ê²°í•˜ëŠ”ê°€?": 10,
        "AI/ML ê¸°ìˆ  í™œìš©ë„ê°€ ë†’ì€ê°€?": 10,
        "ê¸°ìˆ ì˜ í˜ì‹ ì„±ê³¼ ì°¨ë³„í™”ê°€ ìˆëŠ”ê°€?": 10,
        "ê¸°ìˆ ì  êµ¬í˜„ ê°€ëŠ¥ì„±ì´ ë†’ì€ê°€?": 10,
        "ì‹œìŠ¤í…œì˜ í™•ì¥ ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10,
        "ê¸°ìˆ  ì•ˆì •ì„±ê³¼ ë³´ì•ˆì´ í™•ë³´ë˜ì–´ ìˆëŠ”ê°€?": 10,
        "ë°ì´í„° ê¸°ë°˜ í•™ìŠµ ìµœì í™”ê°€ ê°€ëŠ¥í•œê°€?": 10,
        "API ì—°ë™ ë° í™•ì¥ì„±ì´ ë›°ì–´ë‚œê°€?": 10,
        "ê¸°ìˆ  ë¬¸ì„œí™”ê°€ ì˜ ë˜ì–´ ìˆëŠ”ê°€?": 10,
        "ì˜¤í”ˆì†ŒìŠ¤ í™œìš© ë° ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ë„ê°€ ìˆëŠ”ê°€?": 10
    },
    "learning_effectiveness": {
        "í•™ìŠµ ì„±ê³¼ ì¸¡ì • ì§€í‘œê°€ ëª…í™•í•œê°€?": 10,
        "í•™ìŠµì ë§Œì¡±ë„ê°€ ë†’ì€ê°€?": 10,
        "í•™ìŠµ ì™„ë£Œìœ¨ì´ ìš°ìˆ˜í•œê°€?": 10,
        "í•™ìŠµ íš¨ê³¼ ê²€ì¦ ì‚¬ë¡€ê°€ ìˆëŠ”ê°€?": 10,
        "ê°œì¸í™” í•™ìŠµ ì§€ì›ì´ ê°€ëŠ¥í•œê°€?": 10,
        "í•™ìŠµ ë°ì´í„° ë¶„ì„ ë° í”¼ë“œë°± ì œê³µì´ ë˜ëŠ”ê°€?": 10,
        "êµì‚¬/ê°•ì‚¬ ì§€ì› ë„êµ¬ê°€ ìˆëŠ”ê°€?": 10,
        "í•™ìŠµì ì°¸ì—¬ë„ í–¥ìƒ ë°©ì•ˆì´ ìˆëŠ”ê°€?": 10,
        "ì½˜í…ì¸  í’ˆì§ˆì´ ìš°ìˆ˜í•œê°€?": 10,
        "í•™ìŠµ ê²½ë¡œ ì¶”ì²œì´ íš¨ê³¼ì ì¸ê°€?": 10
    },
    "market": {
        "íƒ€ê²Ÿ êµìœ¡ ì‹œì¥ ê·œëª¨ê°€ í°ê°€?": 10,
        "ì‹œì¥ ì„±ì¥ë¥ ì´ ë†’ì€ê°€?": 10,
        "ìˆ˜ìµ ëª¨ë¸ì´ ëª…í™•í•˜ê³  ì‹¤í˜„ ê°€ëŠ¥í•œê°€?": 10,
        "ê³ ê° ê¸°ë°˜(B2B/B2C)ì´ í™•ë³´ë˜ì–´ ìˆëŠ”ê°€?": 10,
        "ê°€ê²© ì „ëµì´ í•©ë¦¬ì ì¸ê°€?": 10,
        "ì‹œì¥ ì§„ì… ì „ëµì´ êµ¬ì²´ì ì¸ê°€?": 10,
        "CACì´ ì ì ˆí•œê°€?": 10,
        "LTVê°€ ë†’ì€ê°€?": 10,
        "íŒŒíŠ¸ë„ˆì‹­ í™•ë³´ ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10,
        "ê¸€ë¡œë²Œ ì‹œì¥ ì§„ì¶œ ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10
    },
    "competition": {
        "ê²½ìŸì‚¬ ëŒ€ë¹„ ëª…í™•í•œ ì°¨ë³„í™” ìš”ì†Œê°€ ìˆëŠ”ê°€?": 10,
        "ì‹œì¥ ì§„ì… ì¥ë²½ì´ ì¡´ì¬í•˜ëŠ”ê°€?": 10,
        "ê²½ìŸ ìš°ìœ„(íŠ¹í—ˆ, ê¸°ìˆ , ë„¤íŠ¸ì›Œí¬)ê°€ ìˆëŠ”ê°€?": 10,
        "ë¸Œëœë“œ ì¸ì§€ë„ê°€ í˜•ì„±ë˜ì–´ ìˆëŠ”ê°€?": 10,
        "ê³ ê° ì¶©ì„±ë„ê°€ ë†’ì€ê°€?": 10,
        "ì„ ì  íš¨ê³¼(First Mover)ê°€ ìˆëŠ”ê°€?": 10,
        "ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ê°€ ì‘ë™í•˜ëŠ”ê°€?": 10,
        "ì „í™˜ ë¹„ìš©(Switching Cost)ì´ ë†’ì€ê°€?": 10,
        "ê²½ìŸì‚¬ ëŒ€ë¹„ ê°€ì„±ë¹„ê°€ ìš°ìˆ˜í•œê°€?": 10,
        "ì§€ì† ê°€ëŠ¥í•œ ê²½ìŸë ¥ì´ ìˆëŠ”ê°€?": 10
    },
    "risk": {
        "ì¬ë¬´ ì•ˆì •ì„±ì´ í™•ë³´ë˜ì–´ ìˆëŠ”ê°€?": 10,
        "ì°½ì—…ì ë° íŒ€ ì—­ëŸ‰ì´ ìš°ìˆ˜í•œê°€?": 10,
        "êµìœ¡ ê·œì œ ë¦¬ìŠ¤í¬ê°€ ë‚®ì€ê°€?": 10,
        "ì‚¬ì—… ì§€ì† ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10,
        "ê¸°ìˆ  ë¦¬ìŠ¤í¬ ëŒ€ì‘ì´ ë˜ëŠ”ê°€?": 10,
        "ë²•ë¥  ë¦¬ìŠ¤í¬(ê°œì¸ì •ë³´ ë“±)ê°€ ë‚®ì€ê°€?": 10,
        "ìš´ì˜ ë¦¬ìŠ¤í¬ê°€ ê´€ë¦¬ë˜ëŠ”ê°€?": 10,
        "ì‹œì¥ ë³€í™” ëŒ€ì‘ë ¥ì´ ìˆëŠ”ê°€?": 10,
        "ì˜ì¡´ì„± ë¦¬ìŠ¤í¬(íŠ¹ì • ê³ ê°/íŒŒíŠ¸ë„ˆ)ê°€ ë‚®ì€ê°€?": 10,
        "ìœ„ê¸° ê´€ë¦¬ ì²´ê³„ê°€ ê°–ì¶°ì ¸ ìˆëŠ”ê°€?": 10
    },
    "growth_potential": {
        "ì‹œì¥ í™•ì¥ ê°€ëŠ¥ì„±ì´ í°ê°€?": 10,
        "ì œí’ˆ ë‹¤ê°í™” ê³„íšì´ ìˆëŠ”ê°€?": 10,
        "ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµì´ êµ¬ì²´ì ì¸ê°€?": 10,
        "íŒŒíŠ¸ë„ˆì‹­ í™•ëŒ€ ê¸°íšŒê°€ ìˆëŠ”ê°€?": 10,
        "M&A ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10,
        "IPO ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10,
        "ìŠ¤ì¼€ì¼ì—… ì¸í”„ë¼ê°€ ì¤€ë¹„ë˜ì–´ ìˆëŠ”ê°€?": 10,
        "íˆ¬ì ìœ ì¹˜ ì´ë ¥ì´ ìˆëŠ”ê°€?": 10,
        "ì„±ì¥ ë¡œë“œë§µì´ ëª…í™•í•œê°€?": 10,
        "10ë°° ì„±ì¥(10x) ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?": 10
    }
}
INVESTMENT_THRESHOLDS = {"íˆ¬ì": 70, "ë³´ë¥˜": 0}

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
def clean_text(txt: str) -> str:
    return re.sub(r"\s+", " ", txt).strip()

def tavily_search_docs(query: str, limit: int = 8) -> List[Document]:
    """Tavily APIë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not TAVILY_API_KEY:
        return []
    try:
        url = "https://api.tavily.com/v1/search"
        params = {"query": query, "max_results": limit, "include_raw_content": True} # ì›ë³¸ ì½˜í…ì¸  í¬í•¨
        headers = {"Authorization": f"Bearer {TAVILY_API_KEY}"}
        res = requests.get(url, params=params, headers=headers, timeout=20)
        res.raise_for_status()
        data = res.json()
        items = data.get("results") or data.get("items") or []
        docs: List[Document] = []
        for it in items:
            title = it.get("title") or it.get("url") or "untitled"
            url_ = it.get("url")
            content = it.get("raw_content") or it.get("content") or it.get("snippet") or ""
            if content:
                docs.append(Document(page_content=clean_text(content)[:15000],
                                     metadata={"url": url_, "title": title}))
        return docs
    except Exception as e:
        print(f"âŒ Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def build_vectorstore(docs: List[Document], save_path: str):
    os.makedirs(save_path, exist_ok=True)
    vs = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))
    vs.save_local(save_path)
    return save_path

def load_vectorstore(path: str):
    return FAISS.load_local(path, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), allow_dangerous_deserialization=True)


# ====================================================
# ìŠ¤íƒ€íŠ¸ì—… í›„ë³´ ì¶”ì¶œ(RAG) & ì„ ì • ì ìˆ˜í™”(í‚¤ì›Œë“œ ê¸°ë°˜)
# ====================================================
CAND_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ë‹¹ì‹ ì€ EdTech VC ìŠ¤ì¹´ìš°í„°ì…ë‹ˆë‹¤. ë¬¸ì„œ ìŠ¤ë‹ˆí«ì„ ê¸°ë°˜ìœ¼ë¡œ 'êµìœ¡(EdTech) ë¶„ì•¼ ìŠ¤íƒ€íŠ¸ì—…' í›„ë³´ë¥¼ ìµœëŒ€ 10ê°œ JSON ë°°ì—´ë¡œ ë½‘ìœ¼ì„¸ìš”. "
     "ê° í•­ëª©ì€ name, url, region, stage, last_funding, blurb(í•œì¤„ì„¤ëª…) í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì˜¤ì§ JSONë§Œ ì¶œë ¥."),
    ("human", "ë¬¸ì„œ ìŠ¤ë‹ˆí«:\n{snippets}")
])

def extract_candidates_with_rag(vs, query="South Korea EdTech startups list 2024 2025") -> List[Dict]:
    retriever = vs.as_retriever(search_kwargs={"k": 8})
    docs = retriever.get_relevant_documents(query)
    if not docs:
        print("âš ï¸ RAG í›„ë³´ ì¶”ì¶œ: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
        return []
    text = "\n\n".join(
        f"- {d.metadata.get('title','')} | {d.metadata.get('url','')}\n{d.page_content[:1500]}"
        for d in docs
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, openai_api_key=OPENAI_API_KEY)
    raw = llm.invoke(CAND_PROMPT.format_messages(snippets=text)).content
    try:
        data = json.loads(raw)
    except Exception:
        try:
            raw = raw[raw.find("["): raw.rfind("]")+1]
            data = json.loads(raw)
        except Exception:
            data = []
    # normalize
    out = []
    for x in data:
        out.append({
            "name": (x.get("name") or "").strip(),
            "url": x.get("url"),
            "region": x.get("region") or "South Korea",
            "stage": x.get("stage"),
            "last_funding": x.get("last_funding"), # [ğŸ’¡] ì˜ë ¸ë˜ ë¶€ë¶„ ë³µêµ¬
            "blurb": x.get("blurb")
        })
    return [c for c in out if c["name"]]

def keyword_score(text: str, keywords: List[str], unit: float = 0.2) -> float:
    count = sum(1 for kw in keywords if re.search(kw, text, re.I))
    return min(count * unit, 1.0)

def selection_score(c: Dict) -> float:
    desc = " ".join(filter(None, [c.get("name",""), c.get("stage",""), c.get("last_funding",""), c.get("region",""), c.get("blurb","")]))
    score = 0.0
    score += 0.25 * keyword_score(desc, ["growth","10x","rapid","scale up","fast","expansion"])
    score += 0.25 * keyword_score(desc, ["AI","LLM","innovative","unique","creative","patent"])
    score += 0.20 * keyword_score(desc, ["global","scalable","leader","first mover","market share","expansion"])
    score -= 0.15 * keyword_score(desc, ["risk","uncertain","volatile","unstable"])
    score += 0.10 * keyword_score(desc, ["VC","venture","angel","series","funding"])
    score += 0.05 * keyword_score(desc, ["IPO","M&A","exit","acquisition"])
    return max(0.0, min(round(score, 3), 1.0))


# =========================
# ë¶„ì„ í”„ë¡¬í”„íŠ¸(6ê°œ ì˜ì—­)
# =========================
PROMPT_ANALYZE = ChatPromptTemplate.from_messages([
    ("system",
     "ë‹¹ì‹ ì€ íˆ¬ìì‹¬ì‚¬ì—­ì…ë‹ˆë‹¤. íšŒì‚¬ì™€ ë¬¸ì„œ ìŠ¤ë‹ˆí«ì„ ê·¼ê±°ë¡œ {category}ì„(ë¥¼) 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , "
     "í•œ ë¬¸ì¥ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”. JSONë§Œ ì¶œë ¥. ì˜ˆ:{\"score\":82,\"evidence\":\"...\"}"),
    ("human", "íšŒì‚¬: {company}\në¬¸ì„œ ìŠ¤ë‹ˆí«:\n{snips}")
])

def rag_score(vs, company: str, category_desc: str, search_q: str) -> Dict:
    retriever = vs.as_retriever(search_kwargs={"k": 8})
    docs = retriever.get_relevant_documents(f"{company} {search_q}")
    snips = "\n\n".join(
        f"- {d.metadata.get('title','')} | {d.metadata.get('url','')}\n{d.page_content[:1500]}"
        for d in docs
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, openai_api_key=OPENAI_API_KEY)
    raw = llm.invoke(PROMPT_ANALYZE.format_messages(company=company, snips=snips, category=category_desc)).content
    try:
        data = json.loads(raw)
    except Exception:
        data = {"score": 60, "evidence": "ì¶œì²˜ ë¶€ì¡±"}
    score = int(max(0, min(100, data.get("score", 60))))
    ev = str(data.get("evidence",""))
    return {"score": score, "evidence": ev}


# =========================
# Node êµ¬í˜„ (Flowchart ë§¤í•‘)
# =========================
def node_web_crawling(state: AgentState) -> Dict: 
    print("ğŸŒ WebCrawling: Tavilyë¡œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ ìˆ˜ì§‘/RAG ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    search_query = "South Korea EdTech startups list 2024 2025"
    
    try:
        docs: List[Document] = []
        if TAVILY_API_KEY:
            print(f"âœ… Tavilyë¡œ '{search_query}' ì •ë³´ ê²€ìƒ‰ ì¤‘...")
            docs = tavily_search_docs(search_query, limit=15) 
            if not docs:
                 print("âš ï¸ Tavily ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                 print(f"âœ… Tavilyì—ì„œ {len(docs)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            print("âŒ TAVILY_API_KEYê°€ ì—†ì–´ í¬ë¡¤ë§í•  ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"vectorstore_path": "error", "candidates": []}

        if not docs:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ì–´ RAG ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"vectorstore_path": "error", "candidates": []}

        vs_path = "vs_edtech_index"
        os.makedirs(vs_path, exist_ok=True)
        vs_path_result = build_vectorstore(docs, vs_path) 
        vs = load_vectorstore(vs_path_result)

        cands = extract_candidates_with_rag(vs, query=search_query) 
        if cands:
            print(f"  â†³ í›„ë³´ {len(cands)}ê°œ ì¶”ì¶œ")
        else:
            print("âš ï¸ í›„ë³´ ì—†ìŒ â€” ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ìë™ ë³´ë¥˜ ì²˜ë¦¬")
            
        return {"vectorstore_path": vs_path_result, "candidates": cands}
        
    except Exception as e:
        print(f"âŒ WebCrawling ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"vectorstore_path": "error", "candidates": []}


def node_filtering(state: AgentState) -> Dict: 
    print("[bold cyan]ğŸ” Filtering: ê¸°ì¤€ ë¶€í•© í›„ë³´ ì„ ë³„ ì¤‘...[/]")
    cands = state.get("candidates", [])
    filtered = [c for c in cands if c.get("name") and (c.get("blurb") or c.get("url"))]
    loop_count = state.get("loop_count", 0) + 1
    
    ranked = sorted(filtered, key=selection_score, reverse=True)
    print(f"  â†³ í•„í„° í†µê³¼ (ì •ë ¬ ì™„ë£Œ): [bold]{len(ranked)}ê°œ[/]")
    
    return {"filtered": ranked, "current_idx": 0, "loop_count": loop_count}


def node_select_one(state: AgentState) -> Dict: 
    filtered = state.get("filtered", [])
    idx = state.get("current_idx", 0)
    
    current_candidate = filtered[idx] if idx < len(filtered) else None
    
    if current_candidate:
        n = current_candidate["name"]
        sc = selection_score(current_candidate)
        print(f"[bold green]âœ… SelectOne:[/] #{idx+1} {n} (ì„ ì •ì ìˆ˜ {sc})")
    else:
        print("[bold yellow]âš ï¸ SelectOne: ë” ì´ìƒ í‰ê°€í•  í›„ë³´ ì—†ìŒ[/]")
        
    return {"current_candidate": current_candidate, "current_idx": idx + 1}


def node_tech_summary(state: AgentState) -> Dict: 
    c = state.get("current_candidate")
    if not c:
        return {"technology_score": 0, "learning_effectiveness_score": 0} 

    try:
        company = c["name"]
        vs = load_vectorstore(state["vectorstore_path"])
        tech = rag_score(vs, company, "ê¸°ìˆ ë ¥(AI/ML í™œìš©, í˜ì‹ ì„±, í™•ì¥ì„±)", "architecture model patent scalability AI LLM")
        learn = rag_score(vs, company, "í•™ìŠµ ì„±ê³¼(í•™ìŠµíš¨ê³¼, ë§Œì¡±ë„, ì™„ë£Œìœ¨)", "learning outcome efficacy satisfaction completion rate case study")
        print(f"[bold cyan]ğŸ§  TechSummary:[/] ê¸°ìˆ  {tech['score']}, í•™ìŠµì„±ê³¼ {learn['score']}")
        return {
            "technology_score": tech["score"],
            "technology_analysis_evidence": tech["evidence"],
            "learning_effectiveness_score": learn["score"],
            "learning_effectiveness_analysis_evidence": learn["evidence"]
        }
    except Exception as e:
        print(f"âŒ TechSummary ì¤‘ ì˜¤ë¥˜: {e}")
        return { 
            "technology_score": 0, "technology_analysis_evidence": f"ë¶„ì„ ì˜¤ë¥˜: {e}",
            "learning_effectiveness_score": 0, "learning_effectiveness_analysis_evidence": f"ë¶„ì„ ì˜¤ë¥˜: {e}"
        }


def node_market_eval(state: AgentState) -> Dict: 
    c = state.get("current_candidate")
    if not c:
        return {"market_score": 0, "growth_potential_score": 0}

    try:
        company = c["name"]
        vs = load_vectorstore(state["vectorstore_path"])
        market = rag_score(vs, company, "ì‹œì¥ì„±(ì‹œì¥ê·œëª¨/ì„±ì¥ë¥ /ìˆ˜ìµëª¨ë¸)", "market size growth TAM SAM SOM revenue model pricing")
        growth = rag_score(vs, company, "ì„±ì¥ ê°€ëŠ¥ì„±(ì‹œì¥ í™•ì¥/ê¸€ë¡œë²Œ ì§„ì¶œ)", "expansion global go-to-market partnership localization")
        print(f"[bold cyan]ğŸ“ˆ MarketEval:[/] ì‹œì¥ {market['score']}, ì„±ì¥ê°€ëŠ¥ì„± {growth['score']}")
        
        return {
            "market_score": market["score"],
            "market_analysis_evidence": market["evidence"],
            "growth_potential_score": growth["score"],
            "growth_potential_analysis_evidence": growth["evidence"]
        }
    except Exception as e:
        print(f"âŒ MarketEval ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "market_score": 0, "market_analysis_evidence": f"ë¶„ì„ ì˜¤ë¥˜: {e}",
            "growth_potential_score": 0, "growth_potential_analysis_evidence": f"ë¶„ì„ ì˜¤ë¥˜: {e}"
        }


def node_comp_analysis(state: AgentState) -> Dict: 
    c = state.get("current_candidate")
    if not c:
        return {"competition_score": 0, "risk_score": 0} 

    try:
        company = c["name"]
        vs = load_vectorstore(state["vectorstore_path"])
        comp = rag_score(vs, company, "ê²½ìŸë ¥(ì°¨ë³„í™”/ê²½ìŸìš°ìœ„/ëª¨ë°©ë‚œì´ë„)", "competitor differentiation moat switching cost")
        risk = rag_score(vs, company, "ë¦¬ìŠ¤í¬(ì¬ë¬´/íŒ€/ê·œì œ)", "risk runway funding debt regulation compliance team risk")
        print(f"[bold cyan]âš”ï¸ CompAnalysis:[/] ê²½ìŸë ¥ {comp['score']}, ë¦¬ìŠ¤í¬ {risk['score']}")
        
        return {
            "competition_score": comp["score"],
            "competition_analysis_evidence": comp["evidence"],
            "risk_score": risk["score"],
            "risk_analysis_evidence": risk["evidence"]
        }
    except Exception as e:
        print(f"âŒ CompAnalysis ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "competition_score": 0, "competition_analysis_evidence": f"ë¶„ì„ ì˜¤ë¥˜: {e}",
            "risk_score": 0, "risk_analysis_evidence": f"ë¶„ì„ ì˜¤ë¥˜: {e}"
        }


def node_invest_decision(state: AgentState) -> Dict: 
    w = {
        "technology_score": 0.20,
        "learning_effectiveness_score": 0.15,
        "market_score": 0.20,
        "competition_score": 0.15,
        "risk_score": 0.10,
        "growth_potential_score": 0.20
    }
    total = sum(state.get(k, 0)*w[k] for k in w.keys())
    judge = "íˆ¬ì" if total >= INVESTMENT_THRESHOLDS["íˆ¬ì"] else "ë³´ë¥˜"
    
    print(f"[bold yellow]ğŸ’¡ InvestDecision:[/] ì´ì  {total:.1f} â†’ {judge}")
    
    return {"final_judge": judge}


def node_hold(state: AgentState) -> Dict: 
    """ë³´ë¥˜ ì‹œ ë‹¤ìŒ í›„ë³´ ë°˜ë³µ (ë‹¨ìˆœíˆ SelectOneìœ¼ë¡œ ë£¨í”„)"""
    print("[bold magenta]â¸ï¸ Hold: ë‹¤ìŒ í›„ë³´ë¡œ ì´ë™[/]")
    return {} # ë¹ˆ ë”•ì…”ë„ˆë¦¬(ìœ íš¨í•œ ì—…ë°ì´íŠ¸)ë¥¼ ë°˜í™˜


def node_report(state: AgentState) -> Dict: 
    c = state.get("current_candidate") or {}
    name = c.get("name", "Unknown")
    md = f"""
[ìŠ¤íƒ€íŠ¸ì—…] {name}

ê¸°ìˆ ë ¥: {state.get('technology_score',0)}ì  â€” {state.get('technology_analysis_evidence','')}
í•™ìŠµ ì„±ê³¼: {state.get('learning_effectiveness_score',0)}ì  â€” {state.get('learning_effectiveness_analysis_evidence','')}
ì‹œì¥ì„±: {state.get('market_score',0)}ì  â€” {state.get('market_analysis_evidence','')}
ê²½ìŸë ¥: {state.get('competition_score',0)}ì  â€” {state.get('competition_analysis_evidence','')}
ë¦¬ìŠ¤í¬(ì•ˆì „ë„): {state.get('risk_score',0)}ì  â€” {state.get('risk_analysis_evidence','')}
ì„±ì¥ ê°€ëŠ¥ì„±: {state.get('growth_potential_score',0)}ì  â€” {state.get('growth_potential_analysis_evidence','')}

â–¶ ìµœì¢… íŒë‹¨: {state.get('final_judge','ë³´ë¥˜')}
""".strip()

    safe_name = re.sub(r'[^0-9A-Za-zê°€-í£_\-]+','_',name) or "Unknown"
    pdf_path = f"{safe_name}_invest_report.pdf"
    styles = getSampleStyleSheet()
    doc = SimpleDocTemplate(pdf_path, pagesize=A4)
    story = [Paragraph(md.replace("\n","<br/>"), styles["Normal"]), Spacer(1,12)]
    doc.build(story)
    
    print(f"[bold green]ğŸ“ Report:[/] PDF ìƒì„± ì™„ë£Œ â†’ {pdf_path}")
    
    return {"report": md, "pdf_path": pdf_path}


# =========================
# ê·¸ë˜í”„ êµ¬ì„± (Flowchart ê·¸ëŒ€ë¡œ)
# =========================
def build_app():
    graph = StateGraph(AgentState)

    graph.add_node("WebCrawling", node_web_crawling)
    graph.add_node("Filtering", node_filtering)
    graph.add_node("SelectOne", node_select_one)
    graph.add_node("TechSummary", node_tech_summary)
    graph.add_node("MarketEval", node_market_eval)
    graph.add_node("CompAnalysis", node_comp_analysis)
    graph.add_node("InvestDecision", node_invest_decision)
    graph.add_node("Hold", node_hold)
    graph.add_node("Report", node_report)

    graph.set_entry_point("WebCrawling")
    graph.add_edge("WebCrawling", "Filtering")

    def guard_filter(state: AgentState):
        return "SelectOne" if state.get("filtered") else "End"
    graph.add_conditional_edges("Filtering", guard_filter, {"SelectOne":"SelectOne", "End": END})
    
    def guard_select(state: AgentState):
        return "Analyze" if state.get("current_candidate") else "End"
    graph.add_conditional_edges("SelectOne", guard_select, {"Analyze": "TechSummary", "End": END})

    graph.add_edge("TechSummary", "MarketEval")
    graph.add_edge("MarketEval", "CompAnalysis")
    graph.add_edge("CompAnalysis", "InvestDecision")

    def guard_decision(state: AgentState):
        return "Report" if state.get("final_judge") == "íˆ¬ì" else "Hold"
    graph.add_conditional_edges("InvestDecision", guard_decision, {"Report":"Report", "Hold":"Hold"})

    graph.add_edge("Hold", "SelectOne") 
    graph.add_edge("Report", END)

    # [ğŸ’¡ğŸ’¡ğŸ’¡] SqliteSaver ê´€ë ¨ ì½”ë“œë¥¼ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤.
    # memory = SqliteSaver.in_memory() 
    
    return graph.compile() # <--- checkpointerê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.


# =========================
# ì‹¤í–‰
# =========================
if __name__ == "__main__":
    
    app = build_app()
    init: Dict = {} 

    # [ğŸ’¡ğŸ’¡ğŸ’¡] checkpointerê°€ ì—†ìœ¼ë¯€ë¡œ thread_idê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    # thread_id = f"edtech_run_{int(time.time())}"
    print(f"ğŸš€ ì‹¤í–‰ ì‹œì‘")

    try:
        # [ğŸ’¡ğŸ’¡ğŸ’¡] config=... ë¶€ë¶„ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
        final_state = app.invoke(init)
        
        print("\n=== ìµœì¢… ê²°ê³¼ ===")
        if final_state.get("report"):
            print(final_state["report"])
            print("PDF:", final_state["pdf_path"])
        else:
            print("ë³´ê³ ì„œ ì—†ì´ ì¢…ë£Œë¨(í›„ë³´ ì—†ìŒ ë˜ëŠ” ëª¨ë‘ ë³´ë¥˜)")
            
    except Exception as e:
        print(f"\nâŒ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
import os
import re
import requests
import pandas as pd
import random
import json
import time # [ğŸ’¡] ëŒ€ê¸° ì‹œê°„ ì‚¬ìš© ìœ„í•´ ì¶”ê°€
from typing import List, Set, Dict, Optional
from dotenv import load_dotenv
from openai import OpenAI
from rich import print

# === â‘  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
load_dotenv()
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not TAVILY_API_KEY:
    raise ValueError("ğŸš¨ Tavily API key missing (.env).")
if not OPENAI_API_KEY:
    raise ValueError("ğŸš¨ OpenAI API key missing (.env).")

client = OpenAI(api_key=OPENAI_API_KEY)
TAVILY_URL = "https://api.tavily.com/search"
STARTUP_CSV_FILE = "ai_filtered_startups.csv"
# [ğŸ’¡ğŸ’¡ğŸ’¡] ìˆœìœ„ ê²°ê³¼ë¥¼ ì €ì¥í•  ìƒˆ CSV íŒŒì¼ëª…
RANKED_CSV_FILE = "ranked_startup_evaluations.csv"

# === â‘¡-1. ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ ìƒì„±ì„ ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ===
AGGREGATION_QUERIES = [
    "AI êµìœ¡ ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ (ì˜ˆ: Riiid, Mathpresso, Sana Labs, Squirrel AI ë“±)",
    "EdTech ë¶„ì•¼ì—ì„œ ì¸ê³µì§€ëŠ¥(AI)ì„ í™œìš©í•˜ëŠ” ì£¼ìš” ìŠ¤íƒ€íŠ¸ì—…",
    "Top AI education or EdTech startups 2025 funding news",
    "AI tutoring platform startups using adaptive learning investment",
]

# === â‘¡-2. ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ ìƒì„±ì„ ìœ„í•œ ì •ê·œì‹ + í•„í„° ===
NAME_PATTERN = re.compile(
    r"\b([A-Z][A-Za-z0-9&'\-]*(?:\s+[A-Z][A-Za-z0-9&'\-]*){0,2})\b"
)
STOPWORDS = {
    "AI", "Labs", "Learning", "Education", "EdTech", "Systems", "Company", "Group",
    "Technology", "Technologies", "Platform", "Startup", "News", "Report", "Top",
    "Software", "Tools", "Adaptive", "Artificial", "Intelligence", "Market",
    "Overview", "Trend", "Global", "Data", "Model", "Classroom", "Program", "School",
    "South", "Korea", "Best", "List", "World",
}

# === â‘¡-3. ìŠ¤íƒ€íŠ¸ì—… í‰ê°€ ê¸°ì¤€ (ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ì¥ ì¤‘ì‹¬) ===
EVALUATION_CRITERIA = {
    "purpose": [  # ëª©ì : ë¹ ë¥¸ í™•ì¥ê³¼ ì‹œì¥ ì„ ì 
        "íšŒì‚¬ì˜ ì£¼ëœ ëª©í‘œê°€ ì‹œì¥ ì ìœ ìœ¨ì˜ ë¹ ë¥¸ í™•ë³´ì¸ê°€?",
        "ë‹¨ê¸°ì ì¸ ìˆ˜ìµì„±ë³´ë‹¤ ì¥ê¸°ì ì¸ ì‹œì¥ ì§€ë°°ë ¥ì„ ìš°ì„ í•˜ëŠ”ê°€?",
        "ê³µê²©ì ì¸ ì„±ì¥ ì „ëµ(ì˜ˆ: ëŒ€ê·œëª¨ ë§ˆì¼€íŒ…, ë¹ ë¥¸ ì œí’ˆ ì¶œì‹œ)ì„ ì¶”êµ¬í•˜ëŠ”ê°€?",
        "ê¸°ì¡´ ì‹œì¥ì„ íŒŒê´´(disrupt)í•˜ë ¤ëŠ” ëª…í™•í•œ ë¹„ì „ì´ ìˆëŠ”ê°€?",
        "ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ë‚˜ ê·œëª¨ì˜ ê²½ì œë¥¼ í†µí•´ ì‹œì¥ì„ ì„ ì í•˜ë ¤ëŠ” ê³„íšì´ ìˆëŠ”ê°€?"
    ],
    "growth_speed": [  # ì„±ì¥ ì†ë„: ìƒë‹¹íˆ ë¹ ë¦„ (10ë°° ì„±ì¥ ë“±)
        "ì‚¬ìš©ì, ë§¤ì¶œ ë“± í•µì‹¬ ì§€í‘œì˜ ì„±ì¥ ëª©í‘œê°€ ë§¤ìš° ë†’ì€ê°€? (ì˜ˆ: ì—° 10ë°°)",
        "ê³¼ê±° ì„±ì¥ë¥ ì´ ì—…ê³„ í‰ê· ì„ í¬ê²Œ ìƒíšŒí–ˆëŠ”ê°€?",
        "ë‹¨ê¸°ê°„ ë‚´ í­ë°œì ì¸ ì„±ì¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì´ ìˆëŠ”ê°€?",
        "ì„±ì¥ ì†ë„ë¥¼ ê°€ì†í™”í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê³„íš(ì˜ˆ: ì¸ì¬ ì˜ì…, ê¸°ìˆ  íˆ¬ì)ì´ ìˆëŠ”ê°€?",
        "ì‹œì¥ ë³€í™”ì— ë¹ ë¥´ê²Œ ì ì‘í•˜ë©° ì„±ì¥ ëª¨ë©˜í…€ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ”ê°€?"
    ],
    "idea": [  # ì•„ì´ë””ì–´: í˜ì‹ ì ì´ê³  ë…ì°½ì ì¸ ê¸°ìˆ , ì„œë¹„ìŠ¤ ì¤‘ì‹¬
        "í•µì‹¬ ê¸°ìˆ ì´ë‚˜ ì„œë¹„ìŠ¤ê°€ ê¸°ì¡´ ë°©ì‹ê³¼ ë¹„êµí•´ ëª…í™•íˆ í˜ì‹ ì ì¸ê°€?",
        "ë…ìì ì¸ ê¸°ìˆ (ì˜ˆ: íŠ¹í—ˆ)ì´ë‚˜ ì°¨ë³„í™”ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ ê°€ì§€ê³  ìˆëŠ”ê°€?",
        "ì•„ì´ë””ì–´ê°€ ëª¨ë°©í•˜ê¸° ì–´ë µê±°ë‚˜ ìƒë‹¹í•œ ì§„ì… ì¥ë²½ì„ ê°€ì§€ê³  ìˆëŠ”ê°€?",
        "ê¸°ìˆ /ì„œë¹„ìŠ¤ê°€ ì ì¬ì ìœ¼ë¡œ ìƒˆë¡œìš´ ì‹œì¥ì„ ì°½ì¶œí•  ìˆ˜ ìˆëŠ”ê°€?",
        "ì•„ì´ë””ì–´ê°€ ëª…í™•í•˜ê³  ì„¤ë“ë ¥ ìˆê²Œ ì „ë‹¬ë˜ëŠ”ê°€?"
    ],
    "uncertainty": [  # ë¶ˆí™•ì‹¤ì„±: ë§¤ìš° ë†’ìŒ (ì‹œì¥, ê¸°ìˆ , ê³ ê° ê´€ì )
        "íƒ€ê²Ÿ ì‹œì¥ì˜ ë°˜ì‘ì´ ì•„ì§ ê²€ì¦ë˜ì§€ ì•Šì•˜ëŠ”ê°€?",
        "í•µì‹¬ ê¸°ìˆ ì´ ìƒìš©í™” ì´ˆê¸° ë‹¨ê³„ì´ê±°ë‚˜ ì•„ì§ ê°œë°œ ì¤‘ì¸ê°€?",
        "ê³ ê°ì˜ í–‰ë™ì´ë‚˜ ë‹ˆì¦ˆ ë³€í™”ì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì–´ë ¤ìš´ê°€?",
        "ê²½ìŸ í™˜ê²½ì´ ë¹ ë¥´ê²Œ ë³€í•˜ê±°ë‚˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ìš”ì†Œê°€ ë§ì€ê°€?",
        "ì‚¬ì—… ëª¨ë¸ì´ë‚˜ ìˆ˜ìµí™” ë°©ì‹ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì´ ì¡´ì¬í•˜ëŠ”ê°€?"
    ],
    "funding": [  # ìê¸ˆ ì¡°ë‹¬: íˆ¬ì ì¤‘ì‹¬ (VC, ì—”ì ¤ ë“±)
        "ì£¼ìš” ìê¸ˆ ì¡°ë‹¬ ë°©ì‹ì´ ì™¸ë¶€ íˆ¬ì ìœ ì¹˜(VC, ì—”ì ¤ ë“±)ì¸ê°€?",
        "ê³¼ê±°ì— ìƒë‹¹ ê·œëª¨ì˜ íˆ¬ìë¥¼ ìœ ì¹˜í•œ ì´ë ¥ì´ ìˆëŠ”ê°€?",
        "í–¥í›„ ëŒ€ê·œëª¨ íˆ¬ì ìœ ì¹˜ë¥¼ ê³„íší•˜ê³  ìˆëŠ”ê°€?",
        "íˆ¬ììë“¤ì´ ë§¤ë ¥ì ìœ¼ë¡œ ëŠë‚„ ë§Œí•œ ì„±ì¥ ìŠ¤í† ë¦¬ë¥¼ ê°€ì§€ê³  ìˆëŠ”ê°€?",
        "ë§¤ì¶œì´ë‚˜ ìì²´ í˜„ê¸ˆ íë¦„ë³´ë‹¤ íˆ¬ìê¸ˆì— ì˜ì¡´í•˜ì—¬ ìš´ì˜ë˜ëŠ” ê²½í–¥ì´ ìˆëŠ”ê°€?"
    ],
    "final_goal": [  # ìµœì¢… ëª©í‘œ: M&A, IPO
        "íšŒì‚¬ì˜ ì¥ê¸°ì ì¸ ëª©í‘œê°€ M&A(ì¸ìˆ˜í•©ë³‘)ì¸ê°€?",
        "íšŒì‚¬ì˜ ì¥ê¸°ì ì¸ ëª©í‘œê°€ IPO(ê¸°ì—…ê³µê°œ)ì¸ê°€?",
        "ì°½ì—…ìë‚˜ ê²½ì˜ì§„ì´ ëª…í™•í•œ Exit ì „ëµì„ ê°€ì§€ê³  ìˆëŠ”ê°€?",
        "M&Aë‚˜ IPOë¥¼ ê°€ëŠ¥í•˜ê²Œ í•  ë§Œí•œ ê·œëª¨ë‚˜ ì‹œì¥ ì§€ìœ„ë¥¼ ëª©í‘œë¡œ í•˜ëŠ”ê°€?",
        "íˆ¬ììë“¤ì´ Exitì„ ê¸°ëŒ€í•  ë§Œí•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ê°€?"
    ]
}


# === â‘¢-1. Tavily ê²€ìƒ‰ í•¨ìˆ˜ (ëª©ë¡ ìƒì„±ìš©) ===
def tavily_search_for_aggregation(query: str, max_results: int = 40) -> dict:
    headers = {"Authorization": f"Bearer {TAVILY_API_KEY}", "Content-Type": "application/json"}
    payload = { "query": query, "max_results": min(max_results, 50), "include_answer": True, "search_depth": "advanced" }
    try:
        res = requests.post(TAVILY_URL, json=payload, headers=headers, timeout=30)
        res.raise_for_status()
        return res.json()
    except requests.exceptions.RequestException as e:
        print(f"âŒ (ëª©ë¡ ìƒì„±) Tavily ê²€ìƒ‰ ì‹¤íŒ¨({query}): {e}")
        return {}
    except Exception as e:
         print(f"âŒ (ëª©ë¡ ìƒì„±) Tavily ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜({query}): {e}")
         return {}

# === â‘¢-2. ì´ë¦„ í›„ë³´ ì¶”ì¶œ í•¨ìˆ˜ ===
def extract_candidate_names(*texts: str) -> Set[str]:
    candidates: Set[str] = set()
    for text in texts:
        if not text: continue
        for match in NAME_PATTERN.findall(text):
            name = match.strip()
            if (len(name) < 3 or name.upper() in STOPWORDS or
                not any(c.islower() for c in name if c.isalpha()) or
                len(name.split()) > 3 or
                not re.match(r"^[A-Z]", name) or
                re.search(r'\d{4}', name)):
                continue
            candidates.add(name)
    return candidates

# === â‘¢-3. AI í•„í„° í•¨ìˆ˜ (GPTë¡œ ì‹¤ì œ ê¸°ì—…ëª…ë§Œ ë‚¨ê¹€) ===
def ai_filter_startups(candidates: List[str]) -> List[str]:
    if not candidates: return []
    prompt = f"""ë‹¤ìŒ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ 'AI ê¸°ë°˜ êµìœ¡(EdTech) ìŠ¤íƒ€íŠ¸ì—…' ë˜ëŠ” ê´€ë ¨ ê¸°ì—…ì˜ ì´ë¦„ë§Œ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ë‰´ìŠ¤ ì œëª©ì˜ ì¼ë¶€, ì¼ë°˜ ëª…ì‚¬, ê¸°ìˆ  ìš©ì–´, ì¸ë¬¼/ë„ì‹œ ì´ë¦„, ë³´ê³ ì„œ ì œëª© ë“±ì€ ëª¨ë‘ ì œì™¸í•˜ê³  ì˜¤ì§ íšŒì‚¬ ì´ë¦„ë§Œ ë‚¨ê²¨ì•¼ í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” íšŒì‚¬ ì´ë¦„ë§Œ í•œ ì¤„ì— í•˜ë‚˜ì”© ë‚˜ì—´í•´ì£¼ì„¸ìš”. ì¤‘ë³µì€ ì œê±°í•´ì£¼ì„¸ìš”.
ë¦¬ìŠ¤íŠ¸: {', '.join(candidates)}"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], temperature=0
        )
        text = response.choices[0].message.content.strip()
        return sorted(list(set(line.strip() for line in text.splitlines() if line.strip() and len(line.strip()) > 1)))
    except Exception as e:
        print(f"âš ï¸ AI í•„í„° ì‹¤íŒ¨: {e}. ì›ë³¸ í›„ë³´ ë°˜í™˜.")
        return sorted(list(set(candidates)))

# === â‘¢-4. ì „ì²´ ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ ìƒì„± ë° ì €ì¥ í•¨ìˆ˜ ===
def generate_and_save_startup_list(output_csv: str) -> List[str]:
    print("[bold blue]=== ğŸš€ 1ë‹¨ê³„: AI EdTech ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ ìƒì„± ì‹œì‘ ===[/]")
    all_candidates: Set[str] = set()
    for q in AGGREGATION_QUERIES:
        print(f"  ğŸ” Searching: {q}")
        data = tavily_search_for_aggregation(q)
        if not data: continue
        all_candidates.update(extract_candidate_names(data.get("answer", "")))
        for r in data.get("results", []):
            all_candidates.update(extract_candidate_names(r.get("title", ""), r.get("content", "")))

    print(f"  ğŸ§© 1ì°¨ ì¶”ì¶œëœ í›„ë³´ ìˆ˜: {len(all_candidates)}")
    if not all_candidates:
         print("  âŒ 1ì°¨ ì¶”ì¶œëœ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
         return []

    print("  ğŸ¤– AI í•„í„°ë§ ì§„í–‰ ì¤‘...")
    filtered_startups = ai_filter_startups(sorted(list(all_candidates)))

    if filtered_startups:
        df = pd.DataFrame(filtered_startups, columns=["startup_name"])
        df.to_csv(output_csv, index=False, encoding="utf-8-sig")
        print(f"  âœ… ìµœì¢… {len(filtered_startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—… ì €ì¥ ì™„ë£Œ â†’ {output_csv}")
        print("  --- ìµœì¢… ëª©ë¡ ---")
        for s in filtered_startups: print(f"  - {s}")
        print("  -----------------")
        return filtered_startups
    else:
        print(f"  âŒ AI í•„í„°ë§ í›„ ë‚¨ì€ ìŠ¤íƒ€íŠ¸ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        pd.DataFrame(columns=["startup_name"]).to_csv(output_csv, index=False, encoding="utf-8-sig")
        return []

# === â‘£-1. Tavilyë¡œ ìƒì„¸ í‰ê°€ìš© ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜ ===
def get_startup_context_for_eval(startup_name: str, max_results: int = 7) -> str:
    print(f"  ğŸ” '{startup_name}' ìƒì„¸ ì •ë³´ ê²€ìƒ‰ ì¤‘ (Tavily)...")
    headers = {"Authorization": f"Bearer {TAVILY_API_KEY}", "Content-Type": "application/json"}
    query = f"{startup_name} EdTech company overview funding technology business model market size competition recent news"
    payload = { "query": query, "max_results": max_results, "include_answer": True, "search_depth": "advanced" }
    try:
        res = requests.post(TAVILY_URL, json=payload, headers=headers, timeout=30)
        res.raise_for_status()
        data = res.json()
        context_parts = []
        if data.get("answer"): context_parts.append(f"Tavily ìš”ì•½:\n{data['answer']}")
        if data.get("results"):
            for i, result in enumerate(data["results"]):
                title = result.get("title", "N/A")
                content = result.get("content", "N/A")
                if content and len(content) > 50:
                    context_parts.append(f"\nì¶œì²˜ {i+1} ({title}):\n{content}")

        if not context_parts:
             print(f"  âš ï¸ '{startup_name}'ì— ëŒ€í•œ ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
             return "ê²€ìƒ‰ëœ ìƒì„¸ ì •ë³´ ì—†ìŒ."

        print(f"  âœ… '{startup_name}' ìƒì„¸ ì •ë³´ ê²€ìƒ‰ ì™„ë£Œ ({len(context_parts)}ê°œ ì¶œì²˜).")
        full_context = "\n".join(context_parts)
        return full_context[:15000]

    except requests.exceptions.RequestException as e:
        print(f"  âŒ (ìƒì„¸ ê²€ìƒ‰) Tavily ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return f"Tavily ìƒì„¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    except Exception as e:
        print(f"  âŒ (ìƒì„¸ ê²€ìƒ‰) Tavily ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"Tavily ìƒì„¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"

# === â‘£-2. AI ìƒì„¸ í‰ê°€ í•¨ìˆ˜ ===
def evaluate_startup_with_ai(startup_name: str, context: str, criteria: Dict[str, List[str]]) -> Optional[Dict]:
    print(f"  ğŸ¤– '{startup_name}' ìƒì„¸ í‰ê°€ ì‹œì‘ (GPT-4o-mini)...")
    criteria_prompt_text = ""
    for category, questions in criteria.items():
        criteria_prompt_text += f"\n### {category.upper()} í‰ê°€ ê¸°ì¤€:\n" + "\n".join(f"- {q}" for q in questions)

    prompt = f"""
ë‹¹ì‹ ì€ ë§¤ìš° ê¼¼ê¼¼í•œ EdTech VC íˆ¬ì ì‹¬ì‚¬ì—­ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìŠ¤íƒ€íŠ¸ì—… '{startup_name}'ì— ëŒ€í•œ ì •ë³´(Context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í‰ê°€ ê¸°ì¤€ë“¤ì„ **ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤**í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.

**Context:**
{context}
---
**í‰ê°€ ê¸°ì¤€:**
{criteria_prompt_text}
---
**ë¶„ì„ ìš”ì²­:**
ìœ„ Contextì™€ í‰ê°€ ê¸°ì¤€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, **ê° 6ê°€ì§€ ì¹´í…Œê³ ë¦¬(technology, learning_effectiveness, market, competition, growth_potential, risk)ë³„**ë¡œ ìŠ¤íƒ€íŠ¸ì—…ì´ ì–¼ë§ˆë‚˜ ìš°ìˆ˜í•œì§€ **ì¢…í•©ì ì¸ ë¶„ì„**ì„ 1-2 ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , **1ì ì—ì„œ 5ì  ì‚¬ì´ì˜ ì ìˆ˜**ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”. (5ì ì´ ê°€ì¥ ìš°ìˆ˜í•¨. ë‹¨, riskëŠ” ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë¦¬ìŠ¤í¬ê°€ ë‚®ìŒì„ ì˜ë¯¸)

**ì¶œë ¥ í˜•ì‹ (ì˜¤ì§ JSON ê°ì²´ë§Œ ì¶œë ¥, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ê¸ˆì§€):**
{{
  "startup_name": "{startup_name}",
  "evaluation_summary": {{
    "technology": {{ "analysis": "...", "score": ì ìˆ˜(1-5) }},
    "learning_effectiveness": {{ "analysis": "...", "score": ì ìˆ˜(1-5) }},
    "market": {{ "analysis": "...", "score": ì ìˆ˜(1-5) }},
    "competition": {{ "analysis": "...", "score": ì ìˆ˜(1-5) }},
    "growth_potential": {{ "analysis": "...", "score": ì ìˆ˜(1-5) }},
    "risk": {{ "analysis": "...", "score": ì ìˆ˜(1-5) }}
  }},
  "overall_assessment": "ì¢…í•© íˆ¬ì ì˜ê²¬..."
}}
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}],
            temperature=0.1, response_format={"type": "json_object"}
        )
        result_json = response.choices[0].message.content
        print(f"  âœ… '{startup_name}' ìƒì„¸ í‰ê°€ ì™„ë£Œ.")
        try:
            parsed_result = json.loads(result_json)
            if "evaluation_summary" in parsed_result and "overall_assessment" in parsed_result:
                 return parsed_result
            else:
                 print(f"  âŒ AI í‰ê°€ JSON êµ¬ì¡° ì˜¤ë¥˜. ì›ë³¸ ì‘ë‹µ:\n{result_json}")
                 return None
        except json.JSONDecodeError as e:
            print(f"  âŒ AI í‰ê°€ JSON íŒŒì‹± ì‹¤íŒ¨: {e}. ì›ë³¸ ì‘ë‹µ:\n{result_json}")
            return None
    except Exception as e:
        print(f"  âŒ AI í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# === â‘¤ ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ğŸ’¡ğŸ’¡ğŸ’¡ ìˆœì°¨ í‰ê°€ -> ìˆœìœ„ ì €ì¥ ë¡œì§ ì¶”ê°€ ğŸ’¡ğŸ’¡ğŸ’¡) ===
if __name__ == "__main__":
    # 1ë‹¨ê³„: ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ ìƒì„± ë° ì €ì¥
    startup_list = generate_and_save_startup_list(STARTUP_CSV_FILE)

    print("\n" + "="*60 + "\n")

    # 2ë‹¨ê³„: ëª©ë¡ì˜ ëª¨ë“  ìŠ¤íƒ€íŠ¸ì—… ìˆœì°¨ í‰ê°€ ë° ê²°ê³¼ ì €ì¥
    all_evaluations: List[Dict] = [] # ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    if not startup_list:
        print("[bold red]â¡ï¸ ìƒì„±ëœ ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ì´ ì—†ì–´ ìƒì„¸ í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/]")
    else:
        print(f"[bold blue]=== âœ¨ 2ë‹¨ê³„: ì´ {len(startup_list)}ê°œ ìŠ¤íƒ€íŠ¸ì—… ìˆœì°¨ ìƒì„¸ í‰ê°€ ì‹œì‘ ===[/]")

        for i, selected_startup in enumerate(startup_list):
            print(f"\n[bold sky_blue1]â­ ({i+1}/{len(startup_list)}) í‰ê°€ ëŒ€ìƒ: {selected_startup} â­[/]")

            startup_context = get_startup_context_for_eval(selected_startup)

            evaluation_result = None # í‰ê°€ ê²°ê³¼ ì´ˆê¸°í™”
            if "ì‹¤íŒ¨" in startup_context or "ì—†ìŒ" in startup_context:
                print("  âŒ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ í‰ê°€ë¥¼ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")
                all_evaluations.append({"startup_name": selected_startup, "error": "Context Retrieval Failed", "total_score": 0})
            else:
                evaluation_result = evaluate_startup_with_ai(
                    selected_startup, startup_context, EVALUATION_CRITERIA
                )

                print("\n" + "-"*50)
                print(f"  [bold green]ğŸ“Š ({i+1}/{len(startup_list)}) í‰ê°€ ê²°ê³¼: {selected_startup} ğŸ“Š[/]")
                if evaluation_result:
                    print(json.dumps(evaluation_result, indent=2, ensure_ascii=False))

                    total_score = 0
                    if evaluation_result.get("evaluation_summary"):
                        for category_data in evaluation_result["evaluation_summary"].values():
                            total_score += category_data.get("score", 0)
                    evaluation_result["total_score"] = total_score
                    all_evaluations.append(evaluation_result)
                    print(f"  [bold yellow]âœ¨ ì´ì : {total_score} / 30 âœ¨[/]")

                else:
                    print("  âŒ í‰ê°€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    all_evaluations.append({"startup_name": selected_startup, "error": "Evaluation Failed", "total_score": 0})
                print("-"*50)

            # API í˜¸ì¶œ ì†ë„ ì¡°ì ˆ (ì˜ˆ: 0.5ì´ˆ ëŒ€ê¸°)
            # print("  â³ ë‹¤ìŒ í‰ê°€ê¹Œì§€ 0.5ì´ˆ ëŒ€ê¸°...")
            # time.sleep(0.5)

    print("\n[bold magenta]=== ğŸ ëª¨ë“  ìŠ¤íƒ€íŠ¸ì—… í‰ê°€ ì™„ë£Œ ===[/]")

    # 3ë‹¨ê³„: í‰ê°€ ê²°ê³¼ ì •ë ¬ ë° CSV ì €ì¥ (ì´ë¦„ê³¼ ì ìˆ˜ë§Œ) + ìµœê³ ì  ìŠ¤íƒ€íŠ¸ì—… ë°œí‘œ
    print("\n" + "="*60 + "\n")
    print(f"[bold blue]=== ğŸ’¾ 3ë‹¨ê³„: í‰ê°€ ê²°ê³¼ ì •ë ¬ ë° {RANKED_CSV_FILE} ì €ì¥ (ì´ë¦„, ì ìˆ˜) ===[/]")

    if not all_evaluations:
        print("âŒ í‰ê°€ëœ ìŠ¤íƒ€íŠ¸ì—…ì´ ì—†ì–´ ê²°ê³¼ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì´ì (total_score) ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_evaluations = sorted(all_evaluations, key=lambda x: x.get("total_score", 0), reverse=True)

        # [ğŸ’¡ğŸ’¡ğŸ’¡] ì´ë¦„ê³¼ ì´ì ë§Œ ì¶”ì¶œí•˜ì—¬ DataFrame ìƒì„±
        ranked_data = [
            {"startup_name": item.get("startup_name"), "total_score": item.get("total_score", 0)}
            for item in sorted_evaluations if "error" not in item # ì˜¤ë¥˜ê°€ ì—†ëŠ” ê²°ê³¼ë§Œ í¬í•¨
        ]

        if not ranked_data:
             print(f"âŒ ìœ íš¨í•œ í‰ê°€ ê²°ê³¼ê°€ ì—†ì–´ {RANKED_CSV_FILE} íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            try:
                df_ranked = pd.DataFrame(ranked_data)
                # CSV íŒŒì¼ë¡œ ì €ì¥ (ì´ë¦„, ì ìˆ˜ë§Œ)
                df_ranked.to_csv(RANKED_CSV_FILE, index=False, encoding="utf-8-sig")
                print(f"âœ… ì´ {len(df_ranked)}ê°œ ìŠ¤íƒ€íŠ¸ì—…ì˜ ì´ë¦„ê³¼ ì ìˆ˜ë¥¼ ìˆœìœ„ëŒ€ë¡œ {RANKED_CSV_FILE}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

                # [ğŸ’¡ğŸ’¡ğŸ’¡] ìµœê³ ì  ìŠ¤íƒ€íŠ¸ì—… ë°œí‘œ (ì •ë ¬ëœ ëª©ë¡ì˜ ì²« ë²ˆì§¸ í•­ëª©ë§Œ)
                top_startup = df_ranked.iloc[0]
                top_score = top_startup["total_score"]

                # ë™ì ìê°€ ìˆëŠ”ì§€ í™•ì¸ (ì •ë³´ ì œê³µ ëª©ì )
                top_startups_df = df_ranked[df_ranked["total_score"] == top_score]

                print(f"\nğŸ… ìµœê³ ì  ìŠ¤íƒ€íŠ¸ì—… (ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ëŒ€ìƒ): [bold yellow]{top_startup['startup_name']}[/] (ì´ì : {int(top_score)} / 30)") # intë¡œ ë³€í™˜í•˜ì—¬ ì†Œìˆ˜ì  ì œê±°

                if len(top_startups_df) > 1:
                    print(f"   (ì°¸ê³ : ì´ {len(top_startups_df)}ê°œì˜ ìŠ¤íƒ€íŠ¸ì—…ì´ ìµœê³ ì  ë™ì ì…ë‹ˆë‹¤. {RANKED_CSV_FILE} ì°¸ì¡°)")

            except Exception as e:
                print(f"âŒ í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print("\n[bold magenta]=== ğŸ‰ íŒŒì´í”„ë¼ì¸ ìµœì¢… ì™„ë£Œ ===[/]")
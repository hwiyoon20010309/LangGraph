import os
import re
import requests
import pandas as pd
from typing import List, Set
from dotenv import load_dotenv
from openai import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# === â‘  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
load_dotenv()
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not TAVILY_API_KEY:
    raise ValueError("ğŸš¨ Tavily API key missing (.env).")
if not OPENAI_API_KEY:
    raise ValueError("ğŸš¨ OpenAI API key missing (.env).")

client = OpenAI(api_key=OPENAI_API_KEY)
TAVILY_URL = "https://api.tavily.com/search"

# === â‘¡ ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì • ===
QUERIES = [
    "AI êµìœ¡ ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ (ì˜ˆ: Riiid, Mathpresso, Sana Labs, Squirrel AI ë“±)",
    "EdTech ë¶„ì•¼ì—ì„œ ì¸ê³µì§€ëŠ¥(AI)ì„ í™œìš©í•˜ëŠ” ì£¼ìš” ìŠ¤íƒ€íŠ¸ì—…",
    "Top AI education or EdTech startups 2025",
    "AI tutoring platform startups using adaptive learning",
]

# === â‘¢ ì •ê·œì‹ + í•„í„° ì„¤ì • ===
NAME_PATTERN = re.compile(r"\b([A-Z][A-Za-z0-9&'\-]*(?:\s+[A-Z][A-Za-z0-9&'\-]*){0,2})\b")

STOPWORDS = {
    "AI", "Labs", "Learning", "Education", "EdTech", "Systems", "Company", "Group",
    "Technology", "Technologies", "Platform", "Startup", "News", "Report", "Top",
    "Software", "Tools", "Adaptive", "Artificial", "Intelligence", "Market",
    "Overview", "Trend", "Global", "Data", "Model", "Classroom", "Program", "School",
}

# === â‘£ Tavily ê²€ìƒ‰ í•¨ìˆ˜ ===
def tavily_search(query: str, max_results: int = 30) -> dict:
    headers = {"Authorization": f"Bearer {TAVILY_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "query": query,
        "max_results": min(max_results, 50),
        "include_answer": True,
        "search_depth": "advanced",
    }
    res = requests.post(TAVILY_URL, json=payload, headers=headers, timeout=30)
    if res.status_code != 200:
        print(f"âŒ Tavily ê²€ìƒ‰ ì‹¤íŒ¨({res.status_code}) â†’ {res.text}")
        return {}
    return res.json()

# === â‘¤ ì´ë¦„ í›„ë³´ ì¶”ì¶œ ===
def extract_candidate_names(*texts: str) -> Set[str]:
    candidates: Set[str] = set()
    for text in texts:
        if not text:
            continue
        for match in NAME_PATTERN.findall(text):
            name = match.strip()
            if len(name) < 3 or name in STOPWORDS:
                continue
            if " " in name and len(name.split()) > 3:
                continue
            if not re.match(r"^[A-Z]", name):
                continue
            candidates.add(name)
    return candidates

# === â‘¥ AI í•„í„° (GPTë¡œ ì‹¤ì œ ê¸°ì—…ëª…ë§Œ ë‚¨ê¹€) ===
def ai_filter_startups(candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    prompt = f"""
ë‹¤ìŒ ë¦¬ìŠ¤íŠ¸ ì¤‘ ì‹¤ì œ 'AI ê¸°ë°˜ êµìœ¡(EdTech) ìŠ¤íƒ€íŠ¸ì—…'ì´ë‚˜ ê¸°ì—… ì´ë¦„ë§Œ ë‚¨ê²¨ì£¼ì„¸ìš”.
ë‰´ìŠ¤ ì œëª©, ê¸°ìˆ ìš©ì–´, ì¼ë°˜ ë‹¨ì–´, ì¸ë¬¼, ë„ì‹œëª…ì€ ëª¨ë‘ ì œì™¸í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ì€ íšŒì‚¬ëª…ë§Œ, í•œ ì¤„ì— í•˜ë‚˜ì”©.
ë¦¬ìŠ¤íŠ¸:
{candidates}
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        text = response.choices[0].message.content.strip()
        return [line.strip() for line in text.splitlines() if line.strip()]
    except Exception as e:
        print("âš ï¸ AI í•„í„° ì‹¤íŒ¨:", e)
        return candidates

# === â‘¦ FAISS ë²¡í„° ì €ì¥ìš© í•¨ìˆ˜ ===
def build_faiss_index(all_texts: List[str], output_path: str = "faiss_index"):
    print(f"ğŸ“¦ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘... ({len(all_texts)}ê°œ ë¬¸ì„œ)")
    docs = [Document(page_content=txt) for txt in all_texts if txt.strip()]
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    vectorstore.save_local(output_path)
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {output_path}/")

# === â‘§ ì „ì²´ ê²€ìƒ‰ ì‹¤í–‰ ===
def aggregate_ai_startups() -> List[str]:
    all_candidates: Set[str] = set()
    all_texts: List[str] = []  # ğŸ”¹ RAGìš© í…ìŠ¤íŠ¸ ì €ì¥
    
    for q in QUERIES:
        print(f"ğŸ” Searching: {q}")
        data = tavily_search(q)
        if not data:
            continue
        
        # ë¬¸ì„œ ë‚´ìš© ì €ì¥ (RAGì— ì‚¬ìš©)
        if "answer" in data:
            all_texts.append(data["answer"])
        for r in data.get("results", []):
            all_texts.append(r.get("title", ""))
            all_texts.append(r.get("content", ""))

        # ê¸°ì—…ëª… ì¶”ì¶œ
        all_candidates.update(extract_candidate_names(data.get("answer", "")))
        for r in data.get("results", []):
            all_candidates.update(extract_candidate_names(r.get("title", ""), r.get("content", "")))

    print(f"ğŸ§© 1ì°¨ ì¶”ì¶œëœ í›„ë³´ ìˆ˜: {len(all_candidates)}")
    
    filtered = ai_filter_startups(sorted(all_candidates))
    
    # ğŸ”¹ RAGìš© FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
    build_faiss_index(all_texts)
    
    return sorted(set(filtered))

# === â‘¨ ì‹¤í–‰ ===
if __name__ == "__main__":
    startups = aggregate_ai_startups()
    if startups:
        df = pd.DataFrame(startups, columns=["startup_name"])
        df.to_csv("ai_filtered_startups.csv", index=False, encoding="utf-8-sig")
        print(f"âœ… ìµœì¢… {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—… ì €ì¥ ì™„ë£Œ â†’ ai_filtered_startups.csv")
        for s in startups:
            print("-", s)
    else:
        print("âŒ ìŠ¤íƒ€íŠ¸ì—…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”.")
